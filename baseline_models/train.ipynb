{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04b86886",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a7274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [23:49:17] Enabling RDKit 2019.09.1 jupyter extensions\n",
      "c:\\Users\\haozh\\.conda\\envs\\deepchem2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from rdkit.Chem import Draw\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "from pickle import dump, load\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "import gzip, pickle\n",
    "from torch_geometric.data import DataLoader\n",
    "import gnn_utils\n",
    "import gnn_model\n",
    "from gnn_model import GNN, BaselineGCN1, BaselineGCN2\n",
    "import config\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f678664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:35:01) [MSC v.1916 64 bit (AMD64)]\n",
      "torch 1.5.0+cu101 cuda 10.1 gpu? True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys,torch;print('py',sys.version);print('torch',torch.__version__,'cuda',torch.version.cuda,'gpu?',torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb512048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start create data\n",
      "start define model\n",
      "start train\n",
      "start epoch 0\n",
      "Validation loss decreased (inf --> 1.502345).  Saving model ...\n",
      "Epoch: 0, Train_rmse: 1.48, Val_rmse: 1.5\n",
      "start epoch 1\n",
      "Validation loss decreased (1.502345 --> 1.430080).  Saving model ...\n",
      "Epoch: 1, Train_rmse: 1.38, Val_rmse: 1.43\n",
      "start epoch 2\n",
      "Validation loss decreased (1.430080 --> 1.123766).  Saving model ...\n",
      "Epoch: 2, Train_rmse: 1.21, Val_rmse: 1.12\n",
      "start epoch 3\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Epoch: 3, Train_rmse: 1.44, Val_rmse: 1.49\n",
      "start epoch 4\n",
      "Validation loss decreased (1.123766 --> 1.093496).  Saving model ...\n",
      "Epoch: 4, Train_rmse: 1.13, Val_rmse: 1.09\n",
      "start epoch 5\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Epoch: 5, Train_rmse: 1.21, Val_rmse: 1.18\n",
      "start epoch 6\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Epoch: 6, Train_rmse: 1.24, Val_rmse: 1.11\n",
      "start epoch 7\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Epoch: 7, Train_rmse: 1.2, Val_rmse: 1.13\n",
      "start epoch 8\n",
      "Validation loss decreased (1.093496 --> 1.060161).  Saving model ...\n",
      "Epoch: 8, Train_rmse: 1.06, Val_rmse: 1.06\n",
      "start epoch 9\n",
      "Validation loss decreased (1.060161 --> 0.979672).  Saving model ...\n",
      "Epoch: 9, Train_rmse: 1.01, Val_rmse: 0.98\n",
      "start epoch 10\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Epoch: 10, Train_rmse: 1.0, Val_rmse: 1.02\n",
      "start epoch 11\n",
      "Validation loss decreased (0.979672 --> 0.930918).  Saving model ...\n",
      "Epoch: 11, Train_rmse: 0.986, Val_rmse: 0.931\n",
      "start epoch 12\n",
      "Validation loss decreased (0.930918 --> 0.917277).  Saving model ...\n",
      "Epoch: 12, Train_rmse: 1.05, Val_rmse: 0.917\n",
      "start epoch 13\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Epoch: 13, Train_rmse: 1.04, Val_rmse: 1.03\n",
      "start epoch 14\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Epoch: 14, Train_rmse: 0.962, Val_rmse: 1.03\n",
      "start epoch 15\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Epoch: 15, Train_rmse: 0.926, Val_rmse: 0.945\n",
      "start epoch 16\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Epoch: 16, Train_rmse: 0.906, Val_rmse: 0.947\n",
      "start epoch 17\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Epoch: 17, Train_rmse: 0.936, Val_rmse: 0.953\n",
      "start epoch 18\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Epoch: 18, Train_rmse: 1.06, Val_rmse: 1.18\n",
      "start epoch 19\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Epoch: 19, Train_rmse: 0.976, Val_rmse: 1.08\n",
      "start epoch 20\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Epoch: 20, Train_rmse: 0.865, Val_rmse: 0.942\n",
      "start epoch 21\n",
      "Validation loss decreased (0.917277 --> 0.897086).  Saving model ...\n",
      "Epoch: 21, Train_rmse: 0.879, Val_rmse: 0.897\n",
      "start epoch 22\n",
      "Validation loss decreased (0.897086 --> 0.843518).  Saving model ...\n",
      "Epoch: 22, Train_rmse: 0.837, Val_rmse: 0.844\n",
      "start epoch 23\n",
      "Validation loss decreased (0.843518 --> 0.787006).  Saving model ...\n",
      "Epoch: 23, Train_rmse: 0.842, Val_rmse: 0.787\n",
      "start epoch 24\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Epoch: 24, Train_rmse: 0.819, Val_rmse: 0.887\n",
      "start epoch 25\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Epoch: 25, Train_rmse: 0.806, Val_rmse: 0.892\n",
      "start epoch 26\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Epoch: 26, Train_rmse: 0.815, Val_rmse: 0.937\n",
      "start epoch 27\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Epoch: 27, Train_rmse: 0.799, Val_rmse: 0.845\n",
      "start epoch 28\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Epoch: 28, Train_rmse: 0.771, Val_rmse: 0.865\n",
      "start epoch 29\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Epoch: 29, Train_rmse: 0.87, Val_rmse: 0.951\n",
      "start epoch 30\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Epoch: 30, Train_rmse: 0.756, Val_rmse: 0.897\n",
      "start epoch 31\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Epoch: 31, Train_rmse: 0.748, Val_rmse: 0.835\n",
      "start epoch 32\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Epoch: 32, Train_rmse: 0.745, Val_rmse: 0.884\n",
      "start epoch 33\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Epoch: 33, Train_rmse: 0.8, Val_rmse: 0.863\n",
      "start epoch 34\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Epoch: 34, Train_rmse: 0.72, Val_rmse: 0.867\n",
      "start epoch 35\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Epoch: 35, Train_rmse: 0.709, Val_rmse: 0.874\n",
      "start epoch 36\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Epoch: 36, Train_rmse: 0.752, Val_rmse: 0.948\n",
      "start epoch 37\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Epoch: 37, Train_rmse: 0.718, Val_rmse: 0.844\n",
      "start epoch 38\n",
      "EarlyStopping counter: 15 out of 25\n",
      "Epoch: 38, Train_rmse: 0.674, Val_rmse: 0.874\n",
      "start epoch 39\n",
      "EarlyStopping counter: 16 out of 25\n",
      "Epoch: 39, Train_rmse: 0.722, Val_rmse: 0.946\n",
      "start epoch 40\n",
      "EarlyStopping counter: 17 out of 25\n",
      "Epoch: 40, Train_rmse: 0.696, Val_rmse: 0.92\n",
      "start epoch 41\n",
      "EarlyStopping counter: 18 out of 25\n",
      "Epoch: 41, Train_rmse: 0.675, Val_rmse: 0.855\n",
      "start epoch 42\n",
      "Validation loss decreased (0.787006 --> 0.780755).  Saving model ...\n",
      "Epoch: 42, Train_rmse: 0.64, Val_rmse: 0.781\n",
      "start epoch 43\n",
      "Validation loss decreased (0.780755 --> 0.750595).  Saving model ...\n",
      "Epoch: 43, Train_rmse: 0.623, Val_rmse: 0.751\n",
      "start epoch 44\n",
      "EarlyStopping counter: 1 out of 25\n",
      "Epoch: 44, Train_rmse: 0.674, Val_rmse: 0.813\n",
      "start epoch 45\n",
      "EarlyStopping counter: 2 out of 25\n",
      "Epoch: 45, Train_rmse: 0.636, Val_rmse: 0.817\n",
      "start epoch 46\n",
      "EarlyStopping counter: 3 out of 25\n",
      "Epoch: 46, Train_rmse: 0.611, Val_rmse: 0.876\n",
      "start epoch 47\n",
      "EarlyStopping counter: 4 out of 25\n",
      "Epoch: 47, Train_rmse: 0.641, Val_rmse: 0.766\n",
      "start epoch 48\n",
      "EarlyStopping counter: 5 out of 25\n",
      "Epoch: 48, Train_rmse: 0.592, Val_rmse: 0.858\n",
      "start epoch 49\n",
      "EarlyStopping counter: 6 out of 25\n",
      "Epoch: 49, Train_rmse: 0.626, Val_rmse: 0.799\n",
      "start epoch 50\n",
      "EarlyStopping counter: 7 out of 25\n",
      "Epoch: 50, Train_rmse: 0.611, Val_rmse: 0.86\n",
      "start epoch 51\n",
      "EarlyStopping counter: 8 out of 25\n",
      "Epoch: 51, Train_rmse: 0.575, Val_rmse: 0.818\n",
      "start epoch 52\n",
      "EarlyStopping counter: 9 out of 25\n",
      "Epoch: 52, Train_rmse: 0.641, Val_rmse: 0.907\n",
      "start epoch 53\n",
      "EarlyStopping counter: 10 out of 25\n",
      "Epoch: 53, Train_rmse: 0.562, Val_rmse: 0.833\n",
      "start epoch 54\n",
      "EarlyStopping counter: 11 out of 25\n",
      "Epoch: 54, Train_rmse: 0.558, Val_rmse: 0.899\n",
      "start epoch 55\n",
      "EarlyStopping counter: 12 out of 25\n",
      "Epoch: 55, Train_rmse: 0.555, Val_rmse: 0.827\n",
      "start epoch 56\n",
      "EarlyStopping counter: 13 out of 25\n",
      "Epoch: 56, Train_rmse: 0.545, Val_rmse: 0.799\n",
      "start epoch 57\n",
      "EarlyStopping counter: 14 out of 25\n",
      "Epoch: 57, Train_rmse: 0.531, Val_rmse: 0.828\n",
      "start epoch 58\n",
      "EarlyStopping counter: 15 out of 25\n",
      "Epoch: 58, Train_rmse: 0.524, Val_rmse: 0.832\n",
      "start epoch 59\n",
      "EarlyStopping counter: 16 out of 25\n",
      "Epoch: 59, Train_rmse: 0.599, Val_rmse: 0.839\n",
      "start epoch 60\n",
      "EarlyStopping counter: 17 out of 25\n",
      "Epoch: 60, Train_rmse: 0.523, Val_rmse: 0.832\n",
      "start epoch 61\n",
      "EarlyStopping counter: 18 out of 25\n",
      "Epoch: 61, Train_rmse: 0.761, Val_rmse: 1.06\n",
      "start epoch 62\n",
      "EarlyStopping counter: 19 out of 25\n",
      "Epoch: 62, Train_rmse: 0.567, Val_rmse: 0.853\n",
      "start epoch 63\n",
      "EarlyStopping counter: 20 out of 25\n",
      "Epoch: 63, Train_rmse: 0.513, Val_rmse: 0.833\n",
      "start epoch 64\n",
      "EarlyStopping counter: 21 out of 25\n",
      "Epoch: 64, Train_rmse: 0.493, Val_rmse: 0.767\n",
      "start epoch 65\n",
      "EarlyStopping counter: 22 out of 25\n",
      "Epoch: 65, Train_rmse: 0.5, Val_rmse: 0.836\n",
      "start epoch 66\n",
      "EarlyStopping counter: 23 out of 25\n",
      "Epoch: 66, Train_rmse: 0.497, Val_rmse: 0.864\n",
      "start epoch 67\n",
      "EarlyStopping counter: 24 out of 25\n",
      "Epoch: 67, Train_rmse: 0.479, Val_rmse: 0.85\n",
      "start epoch 68\n",
      "EarlyStopping counter: 25 out of 25\n",
      "Early stopping\n",
      "training completed at 2025-11-10 00:03:35.058711\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    \n",
    "    # create data\n",
    "    # gnn_utils.create_data()\n",
    "    print(\"start create data\")\n",
    "    with gzip.open(f\"{config.data_dir}train.pkl.gz\", \"rb\") as f:\n",
    "        train_X = pickle.load(f)\n",
    "    with gzip.open(f\"{config.data_dir}val.pkl.gz\", \"rb\") as f:\n",
    "        val_X = pickle.load(f)\n",
    "    with gzip.open(f\"{config.data_dir}test.pkl.gz\", \"rb\") as f:\n",
    "        test_X = pickle.load(f)\n",
    "\n",
    "    print(\"start define model\")\n",
    "    # define model\n",
    "    n_features = config.n_features # number of node features\n",
    "    bs = config.bs\n",
    "\n",
    "    train_loader = DataLoader(train_X, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_X, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_X, batch_size=bs, shuffle=False, drop_last=False)\n",
    "\n",
    "    train_loader_no_shuffle = DataLoader(train_X, batch_size = bs, shuffle=False, drop_last=False)\n",
    "    val_loader_no_shuffle = DataLoader(val_X, batch_size = bs, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BaselineGCN2(n_features = n_features).to(device)\n",
    "    adam = torch.optim.Adam(model.parameters(), lr = config.lr )\n",
    "    optimizer = adam\n",
    "    early_stopping = gnn_utils.EarlyStopping(patience = config.patience, verbose=True, chkpoint_name = config.best_model2)\n",
    "    criterion = nn.MSELoss()\n",
    "    n_epochs = config.max_epochs\n",
    "\n",
    "    print(\"start train\")\n",
    "    # train the model\n",
    "    hist = {\"train_rmse\":[], \"val_rmse\":[]}\n",
    "    for epoch in range(0, n_epochs):\n",
    "        print(\"start epoch\", epoch)\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output.reshape(-1,)\n",
    "\n",
    "            loss = criterion(output, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        train_rmse = gnn_utils.test_fn(train_loader, model, device)\n",
    "        val_rmse = gnn_utils.test_fn(val_loader, model, device)\n",
    "        early_stopping(val_rmse, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        hist[\"train_rmse\"].append(train_rmse)\n",
    "        hist[\"val_rmse\"].append(val_rmse)\n",
    "        print(f'Epoch: {epoch}, Train_rmse: {train_rmse:.3}, Val_rmse: {val_rmse:.3}')\n",
    "\n",
    "    print(f\"training completed at {datetime.datetime.now()}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc96bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_L2_regularization():\n",
    "    \n",
    "    # create data\n",
    "    # gnn_utils.create_data()\n",
    "    print(\"start create data\")\n",
    "    with gzip.open(f\"{config.data_dir}train.pkl.gz\", \"rb\") as f:\n",
    "        train_X = pickle.load(f)\n",
    "    with gzip.open(f\"{config.data_dir}val.pkl.gz\", \"rb\") as f:\n",
    "        val_X = pickle.load(f)\n",
    "    with gzip.open(f\"{config.data_dir}test.pkl.gz\", \"rb\") as f:\n",
    "        test_X = pickle.load(f)\n",
    "\n",
    "    print(\"start define model\")\n",
    "    # define model\n",
    "    n_features = config.n_features # number of node features\n",
    "    bs = config.bs\n",
    "\n",
    "    train_loader = DataLoader(train_X, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_X, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_X, batch_size=bs, shuffle=False, drop_last=False)\n",
    "\n",
    "    train_loader_no_shuffle = DataLoader(train_X, batch_size = bs, shuffle=False, drop_last=False)\n",
    "    val_loader_no_shuffle = DataLoader(val_X, batch_size = bs, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BaselineGCN2(n_features = n_features).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.l2_lambda)\n",
    "    early_stopping = gnn_utils.EarlyStopping(patience = config.patience, verbose=True, chkpoint_name = config.best_model3)\n",
    "    criterion = nn.MSELoss()\n",
    "    n_epochs = config.max_epochs\n",
    "\n",
    "    print(\"start train\")\n",
    "    # train the model\n",
    "    hist = {\"train_rmse\":[], \"val_rmse\":[]}\n",
    "    for epoch in range(0, n_epochs):\n",
    "        print(\"start epoch\", epoch)\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "        for data in train_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            output = output.reshape(-1,)\n",
    "\n",
    "            loss = criterion(output, data.y)\n",
    "            # L2 regularization\n",
    "            l2_reg = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            loss = loss + config.l2_lambda * l2_reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        train_rmse = gnn_utils.test_fn(train_loader, model, device)\n",
    "        val_rmse = gnn_utils.test_fn(val_loader, model, device)\n",
    "        early_stopping(val_rmse, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        hist[\"train_rmse\"].append(train_rmse)\n",
    "        hist[\"val_rmse\"].append(val_rmse)\n",
    "        print(f'Epoch: {epoch}, Train_rmse: {train_rmse:.3}, Val_rmse: {val_rmse:.3}')\n",
    "\n",
    "    print(f\"training completed at {datetime.datetime.now()}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    run_L2_regularization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
